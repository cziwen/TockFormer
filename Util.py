from typing import Optional

import pandas as pd
import numpy as np
import pytz
import torch
import matplotlib.pyplot as plt
import requests

from sklearn.preprocessing import MinMaxScaler
from datetime import datetime, timedelta


def create_sequences (data, seq_length, target_cols=None, scaler=None, scale=True):
    """
    å°† DataFrame æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹çš„åºåˆ—æ ·æœ¬ã€‚æ—¶é—´ä¼šé»˜è®¤æ’é™¤åœ¨å¤–

    å‚æ•°ï¼š
      - data: pd.DataFrameï¼ŒåŒ…å«ç‰¹å¾åˆ—ï¼ˆæ•°æ®å‡è®¾å·²æŒ‰æ—¶é—´é¡ºåºæ’åˆ—ï¼‰
      - seq_length: æ¯ä¸ªåºåˆ—çš„é•¿åº¦
      - target_cols: æŒ‡å®šç”¨äºä½œä¸ºç›®æ ‡çš„åˆ—åï¼Œé»˜è®¤ä¸º Noneï¼Œè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰åˆ—
      - scaler: å¯é€‰ MinMaxScalerï¼Œç”¨äºå…±äº«è®­ç»ƒæ•°æ®çš„ç¼©æ”¾å™¨
      - scale: æ˜¯å¦è¿›è¡Œå½’ä¸€åŒ–ï¼ˆé»˜è®¤ Trueï¼‰

    è¿”å›ï¼š
      - X_Tensor (samples, seq_length, F)
      - y_Tensor (samples, T)
      - scalerï¼ˆMinMaxScaler å®ä¾‹æˆ– Noneï¼‰
      - target_indicesï¼ˆç”¨äº inverse_transformï¼‰
    """
    feature_columns = data.columns.tolist ()[1:]  # æ’é™¤ç¬¬ä¸€åˆ—æ—¶é—´æˆ³
    df_copy = data.copy ()
    df_copy[df_copy.columns[1:]] = df_copy[df_copy.columns[1:]].astype (np.float32)

    # ======== ä»…åœ¨ scale=True æ—¶æ‰§è¡Œç¼©æ”¾ ========
    if scale:
        print ("æ•°æ®è¢«ç¼©æ”¾")
        if scaler is None:
            scaler = MinMaxScaler (feature_range=(0, 1))
            scaler.fit (df_copy.iloc[:, 1:])  # åª fit ç‰¹å¾åˆ—
        df_copy.iloc[:, 1:] = scaler.transform (df_copy.iloc[:, 1:]).astype (np.float32)
    else:
        print ("æ•°æ®ä¸ç¼©æ”¾")
        scaler = None  # ä¸ç¼©æ”¾æ—¶ï¼Œä¸è¿”å› scaler
    # ==================================================

    data_array = df_copy[feature_columns].values

    if target_cols is None:
        target_indices = list (range (len (feature_columns)))
    elif isinstance (target_cols, str):
        target_indices = [df_copy.columns.get_loc (target_cols) - 1]  # -1 å› ä¸ºå»æ‰äº†æ—¶é—´åˆ—
    elif isinstance (target_cols, list):
        target_indices = [df_copy.columns.get_loc (col) - 1 for col in target_cols]
    else:
        raise ValueError ("target_cols å‚æ•°å¿…é¡»ä¸º None, str æˆ– list")

    X, y = [], []
    for i in range (len (data_array) - seq_length):
        X.append (data_array[i: i + seq_length])
        y.append (data_array[i + seq_length][target_indices])

    X_tensor = torch.tensor (np.array (X), dtype=torch.float32)
    y_tensor = torch.tensor (np.array (y), dtype=torch.float32)

    return X_tensor, y_tensor, scaler, target_indices


def create_prediction_sequence (data, seq_length, scaler, target_col):
    """
    å°† DataFrame æ•°æ®è½¬æ¢ä¸ºé¢„æµ‹åºåˆ—æ ·æœ¬ï¼Œä»…è¿”å› X_tensor å’Œ target_indicesã€‚
    å‡è®¾ç¬¬ä¸€åˆ—ä¸ºæ—¶é—´æˆ³ï¼Œå…¶ä½™åˆ—ä¸ºç‰¹å¾ï¼Œä½¿ç”¨ä¼ å…¥çš„ scaler å¯¹ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–ã€‚

    å‚æ•°ï¼š
      - data: pd.DataFrameï¼Œå·²æŒ‰æ—¶é—´é¡ºåºæ’åˆ—ï¼Œç¬¬ä¸€åˆ—ä¸ºæ—¶é—´æˆ³
      - seq_length: è¾“å…¥åºåˆ—é•¿åº¦
      - scaler: å·² fit çš„ MinMaxScalerï¼Œç”¨äºå½’ä¸€åŒ–
      - target_col: å•ä¸ªåˆ—åæˆ–åˆ—ååˆ—è¡¨ï¼Œç”¨äºåç»­é€†ç¼©æ”¾é¢„æµ‹å€¼

    è¿”å›ï¼š
      - X_tensor: torch.Tensorï¼Œå½¢çŠ¶ä¸º [1, seq_length, F]
      - target_indices: List[int]ï¼Œåœ¨ scaler ä¸­å¯¹åº”çš„ç›®æ ‡åˆ—ç´¢å¼•
    """
    # æå–ç‰¹å¾åˆ—åï¼ˆæ’é™¤æ—¶é—´æˆ³ï¼‰
    feature_columns = data.columns.tolist ()[1:]
    df_copy = data.copy ()
    df_copy[df_copy.columns[1:]] = df_copy[df_copy.columns[1:]].astype (np.float32)

    # æ‰§è¡Œå½’ä¸€åŒ–
    df_copy.iloc[:, 1:] = scaler.transform (df_copy.iloc[:, 1:]).astype (np.float32)
    data_array = df_copy[feature_columns].values

    if len (data_array) < seq_length:
        raise ValueError ("æ•°æ®é•¿åº¦å°äºåºåˆ—é•¿åº¦")

    # æ„é€ è¾“å…¥åºåˆ—
    X = data_array[-seq_length:]
    X_tensor = torch.tensor (X, dtype=torch.float32).unsqueeze (0)  # [1, seq_length, F]

    # è·å– target_indicesï¼ˆåœ¨ scaler ç‰¹å¾ä¸­çš„åˆ—ä½ç½®ï¼‰
    if isinstance (target_col, str):
        target_indices = [df_copy.columns.get_loc (target_col) - 1]  # -1 æ˜¯å› ä¸ºæ’é™¤äº†æ—¶é—´æˆ³
    elif isinstance (target_col, list):
        target_indices = [df_copy.columns.get_loc (col) - 1 for col in target_col]
    else:
        raise ValueError ("target_col å¿…é¡»æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨")

    return X_tensor, target_indices


def plot_metric (values, y_label='Value', title='Training Metric', color='blue', show=True):
    """
    ç»˜åˆ¶æŠ˜çº¿å›¾

    å‚æ•°ï¼š
      - values: æ•°å€¼åˆ—è¡¨æˆ–æ•°ç»„ï¼ˆy è½´æ•°æ®ï¼‰ï¼Œä¾‹å¦‚è®­ç»ƒæŸå¤±æˆ–å‡†ç¡®ç‡ã€‚åˆ—è¡¨ä¸­ç¬¬ 0 ä¸ªå€¼å¯¹åº”ç¬¬ 1 ä¸ª epochï¼Œä»¥æ­¤ç±»æ¨ã€‚
      - y_label: y è½´æ ‡ç­¾ï¼Œé»˜è®¤ä¸º 'Value'
      - title: å›¾åƒæ ‡é¢˜ï¼Œé»˜è®¤ä¸º 'Training Metric'
      - color: çº¿æ¡é¢œè‰²ï¼Œé»˜è®¤ä¸º 'blue'
      - show: æ˜¯å¦æ˜¾ç¤ºå›¾åƒï¼Œé»˜è®¤ä¸º True

    ç¤ºä¾‹ï¼š
    >>> loss_values = [0.9, 0.8, 0.7, 0.65, 0.6]
    >>> plot_metric(loss_values, y_label='Loss', title='Training Loss over Epochs', color='red')
    """
    epochs = range (1, len (values) + 1)  # æ„é€  x è½´ï¼Œä» 1 åˆ° len(values)
    plt.figure (figsize=(8, 6))
    plt.plot (epochs, values, marker='o', linestyle='-', color=color)
    plt.xlabel ('Num Epoch')
    plt.ylabel (y_label)
    plt.title (title)
    plt.grid (True)

    if show:
        plt.show ()


def plot_multiple_curves (accuracy_curve_dict, x_label='period', y_label='Value', title='Training Metric'):
    """
    ç»˜åˆ¶å¤šä¸ªæ›²çº¿ï¼Œå¹¶å åŠ åœ¨åŒä¸€ä¸ªå›¾ä¸Šã€‚

    å‚æ•°:
        accuracy_curve_dict (dict):
            - key: ä»£è¡¨ä¸åŒå®éªŒæˆ–æ¨¡å‹çš„æ ‡ç­¾ (str)
            - value: å¯¹åº”çš„ accuracy æ›²çº¿æ•°æ® (list æˆ– np.array)ï¼Œé•¿åº¦ç­‰äºè®­ç»ƒ epoch æ•°
    """
    plt.figure (figsize=(8, 5))

    # ç”Ÿæˆé¢œè‰²åºåˆ—
    colors = plt.cm.tab20 (np.linspace (0, 1, len (accuracy_curve_dict)))

    for i, (label, acc_curve) in enumerate (accuracy_curve_dict.items ()):
        plt.plot (range (1, len (acc_curve) + 1),
                  acc_curve,
                  # marker='o',
                  # markersize=8,  # å¢å¤§æ ‡è®°å¤§å°
                  linestyle='-',
                  linewidth=1,  # å¢åŠ çº¿å®½
                  label=label,
                  color=colors[i % 10])

    plt.xlabel (x_label)
    plt.ylabel (y_label)
    plt.title (title)
    plt.legend ()
    plt.grid (True)
    plt.show ()


def safeLoadCSV (df):
    # æ£€æŸ¥dataå‰å‡ ä¸ªæ•°æ®æ˜¯å¦æœ‰nanï¼Œå¦‚æœæœ‰ï¼Œåˆ™èˆå¼ƒå‰nè¡Œã€‚
    if df.isna ().any ().any ():  # æ£€æŸ¥æ˜¯å¦æœ‰ NaN
        first_valid_index = df.dropna ().index[0]  # æ‰¾åˆ°ç¬¬ä¸€ä¸ªé NaN æ•°æ®çš„ç´¢å¼•
        df = df.loc[first_valid_index:]  # ä¸¢å¼ƒ NaN ä¹‹å‰çš„æ•°æ®
        df = df.dropna ().reset_index (drop=True)  # é‡æ–°ç´¢å¼•ï¼Œç¡®ä¿è¿ç»­æ€§

    return df


def display_prediction (pred, base_time=None, resolution_minutes=5):
    """
    ç¾è§‚æ‰“å°æ¨¡å‹é¢„æµ‹çš„ OHLC ä»·æ ¼ã€‚

    å‚æ•°ï¼š
      - pred: numpy array æˆ– listï¼Œå½¢çŠ¶ä¸º [1, 4]ï¼ŒåŒ…å«é¢„æµ‹çš„ open, high, low, close
      - base_time: datetimeï¼Œå¯é€‰ï¼Œé¢„æµ‹åŸºå‡†æ—¶é—´ï¼Œè‹¥ä¸º None åˆ™ä½¿ç”¨å½“å‰æ—¶é—´
      - resolution_minutes: intï¼Œé¢„æµ‹æ­¥é•¿çš„åˆ†é’Ÿæ•°ï¼Œé»˜è®¤ä¸º 5 åˆ†é’Ÿ

    è¾“å‡ºï¼š
      - ç¾è§‚çš„é¢„æµ‹ä¿¡æ¯æ‰“å°
    """
    # å¤„ç†æ—¶é—´
    if base_time is None:
        base_time = datetime.now ()
    prediction_time = base_time + timedelta (minutes=resolution_minutes)

    # å¤„ç†é¢„æµ‹å€¼
    pred = pred.flatten ()
    price_dict = {
        "open": round (pred[0], 4),
        "high": round (pred[1], 4),
        "low": round (pred[2], 4),
        "close": round (pred[3], 4),
    }

    # æ‰“å°
    print ("=" * 40)
    print (f"ğŸ“… é¢„æµ‹æ—¶é—´ï¼š{prediction_time.strftime ('%Y-%m-%d %H:%M:%S')}")
    print ("ğŸ“ˆ é¢„æµ‹ä»·æ ¼ï¼ˆå•ä½ï¼šUSDï¼‰ï¼š")
    for k, v in price_dict.items ():
        print (f"  â€¢ {k:<6}: {v:.2f}")
    print ("=" * 40)


def fetch_latest_agg_data (
        ticker: str,
        timespan: str = "second",
        limit: int = 10,
        api_key: str = "your_api_key",
        delayed: bool = False
) -> Optional[pd.DataFrame]:
    """
    è·å–æœ€æ–° N æ¡èšåˆæ•°æ®ï¼Œæ”¯æŒ Free Planï¼ˆ15åˆ†é’Ÿå»¶è¿Ÿæ•°æ®ï¼‰ã€‚

    å‚æ•°:
        ticker: è‚¡ç¥¨ä»£ç 
        timespan: èšåˆå‘¨æœŸï¼ˆsecond, minute, hour, dayï¼‰
        limit: è¿”å›æ¡æ•°
        api_key: Polygon API Key
        delayed: å¦‚æœä¸º Trueï¼Œåˆ™ä½¿ç”¨ç¾å›½ä¸œéƒ¨æ—¶é—´ï¼Œå¹¶å›é€€ 15 åˆ†é’Ÿ

    è¿”å›:
        èšåˆæ•°æ® DataFrameï¼ŒåŒ…å« timestamp, open, high, low, close, volume, vwap
    """
    eastern = pytz.timezone ("America/New_York")
    now_et = datetime.now (eastern)

    if delayed:
        to_time = now_et - timedelta (minutes=15)
        print (f"âš  ä½¿ç”¨å»¶è¿Ÿæ•°æ®æ¨¡å¼ï¼Œto_timeï¼ˆçº½çº¦æ—¶é—´ï¼‰= {to_time}")
    else:
        to_time = now_et

    # è®¡ç®— from_time
    if timespan == "second":
        from_time = to_time - timedelta (seconds=limit)
    elif timespan == "minute":
        from_time = to_time - timedelta (minutes=limit)
    elif timespan == "hour":
        from_time = to_time - timedelta (hours=limit)
    elif timespan == "day":
        from_time = to_time - timedelta (days=limit)
    else:
        raise ValueError (f"Unsupported timespan: {timespan}")

    # è½¬æ¢ä¸º ISO æ ¼å¼ï¼ˆUTC æ—¶é—´ï¼ŒPolygon æ¥æ”¶ ISO8601ï¼‰
    from_utc = from_time.astimezone (pytz.utc).strftime ("%Y-%m-%dT%H:%M:%S")
    to_utc = to_time.astimezone (pytz.utc).strftime ("%Y-%m-%dT%H:%M:%S")

    url = f"https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/{timespan}/{from_utc}/{to_utc}"
    params = {
        "adjusted": "true",
        "sort": "asc",
        "limit": limit,
        "apiKey": api_key
    }

    response = requests.get (url, params=params)
    if response.status_code != 200:
        print (f"è¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")
        return None

    results = response.json ().get ("results", [])
    if not results:
        print ("æœªè·å–åˆ°èšåˆæ•°æ®")
        return None

    df = pd.DataFrame (results)
    df["timestamp"] = pd.to_datetime (df["t"], unit="ms")
    df.rename (columns={
        "o": "open",
        "h": "high",
        "l": "low",
        "c": "close",
        "v": "volume",
        "vw": "vwap"
    }, inplace=True)

    return df[["timestamp", "open", "high", "low", "close", "volume", "vwap"]]


symbol = 'SPY'
API_KEY = "oilTTMMexxTBTmjivaMq3R0Y9ZS1BKbK"

df_min_past = fetch_latest_agg_data (ticker=symbol, timespan="minute", limit=32,
                                     api_key=API_KEY, delayed=True)
# df_sec_past = fetch_latest_agg_data (ticker=symbol, timespan="second", limit=32,
#                                      api_key=API_KEY, delayed=True)
