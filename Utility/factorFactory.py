import pandas as pd
import numpy as np
import math
import matplotlib.pyplot as plt
import umap
import warnings

from typing import List, Optional, Dict, Any
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed

from sklearn.impute import SimpleImputer
from tqdm import tqdm
from scipy.stats import spearmanr
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from mpi4py import MPI
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.model_selection import ParameterGrid
from sklearn.metrics import (
    silhouette_score,
    calinski_harabasz_score,
    davies_bouldin_score,
    silhouette_samples
)
from sklearn.exceptions import ConvergenceWarning
from sklearn.cluster import (
    KMeans,
    AgglomerativeClustering,
    SpectralClustering,
    DBSCAN,
    MeanShift,
    estimate_bandwidth
)
from sklearn.mixture import GaussianMixture

from Utility.registry import FACTOR_REGISTRY
from Utility.factors import *


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ operator functions ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def mul_func (x, y):    return x * y


def minus_func (x, y):  return x - y


def div_func (x, y):    return x.div (y.replace (0, pd.NA))


def sin_func (x):       return x.map (math.sin)


def cos_func (x):       return x.map (math.cos)


CROSS_OPS = [
    {'name': 'mul', 'arity': 2, 'func': mul_func, 'preserves_bounded': True},
    {'name': 'minus', 'arity': 2, 'func': minus_func, 'preserves_bounded': True},
    {'name': 'div', 'arity': 2, 'func': div_func, 'preserves_bounded': False},
    {'name': 'sin', 'arity': 1, 'func': sin_func, 'preserves_bounded': True},
    {'name': 'cos', 'arity': 1, 'func': cos_func, 'preserves_bounded': True},
]


# ================ Pickle Function ================

def _cross_apply (args):
    op, a, b, data_dict = args
    name = f"{a}_{op['name']}_{b}"
    return name, op['func'] (data_dict[a], data_dict[b])


def _evaluate_clustering_task (algo_name, Est, params, X, metrics):
    """
    È°∂Á∫ßÂáΩÊï∞ÔºöËØÑ‰º∞Âçï‰∏™ÁÆóÊ≥ï+ÂèÇÊï∞ÁªÑÂêà„ÄÇ
    ËøîÂõû dict Êàñ None„ÄÇ
    """
    # ÈôêÂà∂Á∞áÊï∞‰∏çË∂ÖËøáÊ†∑Êú¨Êï∞-1
    if 'n_clusters' in params and params['n_clusters'] > X.shape[0] - 1:
        return None
    try:
        with warnings.catch_warnings ():
            warnings.simplefilter ('error', UserWarning)
            warnings.simplefilter ('error', ConvergenceWarning)
            if Est is MeanShift and params.get ('bandwidth') == 'estimate':
                bw = estimate_bandwidth (X)
                model = Est (bandwidth=bw)
            else:
                model = Est (**params)
            labels = model.fit_predict (X) if hasattr (model, 'fit_predict') else model.fit (X).predict (X)
        if len (set (labels)) < 2:
            return None
        rec = {'algo': algo_name}
        rec.update (params)
        if 'silhouette' in metrics:
            rec['silhouette'] = silhouette_score (X, labels)
        if 'calinski_harabasz' in metrics:
            rec['calinski_harabasz'] = calinski_harabasz_score (X, labels)
        if 'davies_bouldin' in metrics:
            rec['davies_bouldin'] = davies_bouldin_score (X, labels)
        rec['_labels'] = labels
        return rec
    except (UserWarning, ConvergenceWarning):
        return None


class FactorFactory:
    def __init__ (
            self,
            df: pd.DataFrame,
            base_cols: Optional[List[str]] = None,
            target_col: str = 'close',
            forward_period: int = 1,
            window: int = 20,
            scaler: str = 'minmax',
            top_k: Optional[int] = None,
            decimals: int = 4
    ):
        # ÂéüÂßãÊï∞ÊçÆÊåâ timestamp Á¥¢Âºï
        self.df_global = (
            df.copy ()
            .sort_values ('timestamp')
            .set_index ('timestamp')
        )
        self.base_cols = (
            base_cols if base_cols is not None
            else [c for c in self.df_global.select_dtypes (include=[np.number]).columns]
        )
        self.target_col = target_col
        self._eval_kwargs = dict (
            forward_period=forward_period,
            window=window,
            scaler=scaler,
            top_k=top_k
        )
        self.decimals = decimals
        self.df_features = pd.DataFrame ()
        self.summary = pd.DataFrame ()
        self.cluster_report = pd.DataFrame ()

    def apply_registry (
            self,
            df: pd.DataFrame,
            cols: List[str],
            bounded_only: bool = False
    ) -> pd.DataFrame:
        """
        ÂØπ‰º†ÂÖ•ÁöÑ dfÔºàÂøÖÈ°ªÊúâ timestamp Á¥¢ÂºïÔºâÊåâÂàó‰æùÊ¨°ÊâßË°å FACTOR_REGISTRY
        ‰∏≠Ê≥®ÂÜåÁöÑÊâÄÊúâ func„ÄÇÂõ†Â≠êÂáΩÊï∞Ëá™Ë°åÂëΩÂêç out[key]ÔºåËøôÈáåÁõ¥Êé•Ê±áÊÄª„ÄÇ
        """
        # Â§á‰ªΩÂéüÂ±ûÊÄß
        orig_df, orig_base, orig_target = self.df_global, self.base_cols, self.target_col

        # ‰∏¥Êó∂ÊõøÊç¢
        self.df_global = df.copy ()
        self.base_cols = cols.copy ()
        self.target_col = self.target_col if self.target_col in df.columns else None

        new_feats: Dict[str, pd.Series] = {}
        for prefix, info in FACTOR_REGISTRY.items ():
            tags = info.get ('tags', [])
            if bounded_only and 'bounded' not in tags:
                continue

            out = info['func'] (self, df, cols)
            for key, series in out.items ():
                # trust the key from factors.py
                new_feats[key] = pd.Series (series, index=df.index, name=key)

        # ÊÅ¢Â§çÂéüÂ±ûÊÄß
        self.df_global, self.base_cols, self.target_col = orig_df, orig_base, orig_target
        return pd.DataFrame (new_feats, index=df.index)

    def generate_factors (
            self,
            mode: str = 'single',
            n_job: Optional[int] = None,
            bounded_only: bool = False
    ) -> pd.DataFrame:
        # 1) ‰∏ÄÈò∂Âõ†Â≠ê
        feat_df = self.apply_registry (
            df=self.df_global,
            cols=self.base_cols,
            bounded_only=bounded_only
        )
        feat_df.index.name = 'timestamp'

        # 2) ‰∫§ÂèâÁâπÂæÅ
        cross_df = self.cross_op (feat_df, mode, n_job, bounded_only)

        # 3) ÂêàÂπ∂ & Ê∏ÖÊ¥ó
        merged = pd.concat ([feat_df, cross_df], axis=1)
        drop_cols = merged.nunique ()[merged.nunique () <= 1].index.tolist ()
        df_feat = merged.drop (columns=drop_cols).dropna ()

        # 4) Â≠òÂÇ® & ËØÑ‰º∞
        self.df_features = df_feat.reset_index ().round (self.decimals)
        self.evaluate_factors (**self._eval_kwargs)
        return self.df_features

    def cross_op (
            self,
            input_df: pd.DataFrame,
            mode: str = 'single',
            n_job: Optional[int] = None,
            bounded_only: bool = False
    ) -> pd.DataFrame:
        data = input_df.copy ()
        feats: Dict[str, pd.Series] = {}

        # ‚Äî‚Äî ‰∏ÄÈò∂Âõ†Â≠ê on Â≠êÈõÜ ‚Äî‚Äî
        reg_df = self.apply_registry (
            df=data,
            cols=list (data.columns),
            bounded_only=bounded_only
        )
        feats.update (reg_df.to_dict ('series'))

        # ‚Äî‚Äî ‰∏ÄÂÖÉÁÆóÂ≠ê ‚Äî‚Äî
        for col in data.columns:
            for op in CROSS_OPS:
                if op['arity'] == 1 and (not bounded_only or op['preserves_bounded']):
                    feats[f"{op['name']}_{col}"] = op['func'] (data[col])

        # ‚Äî‚Äî ‰∫åÂÖÉÁÆóÂ≠êÊûÑÈÄ†‰ªªÂä°ÂàóË°® ‚Äî‚Äî
        dd = data.to_dict ('series')
        tasks = [
            (op, a, b, dd)
            for op in CROSS_OPS
            if op['arity'] == 2 and (not bounded_only or op['preserves_bounded'])
            for a in data.columns for b in data.columns
        ]

        # Âπ∂Ë°åÊàñ‰∏≤Ë°åÊâßË°å
        if mode == 'mpi':
            comm = MPI.COMM_WORLD
            size, rank = comm.Get_size (), comm.Get_rank ()
            local = [t for i, t in enumerate (tasks) if i % size == rank]
            local_res = dict (_cross_apply (t) for t in local)
            all_res = comm.gather (local_res, root=0)
            if rank == 0:
                merged = {}
                for d in all_res: merged.update (d)
                feats.update (merged)
            comm.Barrier ()

        elif mode == 'process':
            with ProcessPoolExecutor (max_workers=n_job) as exe:
                for name, series in tqdm (exe.map (_cross_apply, tasks),
                                          total=len (tasks), desc="üîÑ cross_op (proc)"):
                    feats[name] = series

        elif mode == 'thread':
            with ThreadPoolExecutor (max_workers=n_job) as exe:
                for name, series in tqdm (exe.map (_cross_apply, tasks),
                                          total=len (tasks), desc="üîÑ cross_op (thread)"):
                    feats[name] = series

        else:  # single
            for t in tqdm (tasks, total=len (tasks), desc="üîÑ cross_op (single)"):
                name, series = _cross_apply (t)
                feats[name] = series

        df_new = pd.DataFrame (feats).reindex (data.index)
        df_new.index.name = 'timestamp'
        return df_new

    def evaluate_factors (
            self,
            forward_period: int,
            window: int,
            scaler: str = 'minmax',
            top_k: Optional[int] = None
    ) -> pd.DataFrame:
        df = self.df_features.set_index ('timestamp')
        price = self.df_global[self.target_col]
        returns = price.shift (-forward_period) / price - 1
        df_eval = df.join (returns.rename ('forward_return')).dropna ()

        win_eff = min (window, max (1, len (df_eval) - 1))

        stats: Dict[str, Dict[str, float]] = {}
        for col in df_eval.columns.drop ('forward_return'):
            raw_sp = spearmanr (df_eval[col], df_eval['forward_return']).correlation
            sp = float (np.atleast_1d (raw_sp).ravel ()[0])

            roll = df_eval[col].rolling (win_eff).corr (df_eval['forward_return'])
            valid = roll.dropna ().values
            ir = float (valid.mean () / valid.std (ddof=0)) if valid.size else np.nan

            stats[col] = {'spearman_ic': sp, 'pearson_ir': ir}

        X_sub = (df[list (stats)] - df[list (stats)].mean ()) / df[list (stats)].std ()
        imputer = SimpleImputer(strategy='mean')
        X_sub = imputer.fit_transform (X_sub)
        pca = PCA (n_components=1)
        pca.fit (X_sub)
        for f, ld in zip (stats.keys (), np.abs (pca.components_[0])):
            stats[f]['pca_coeff'] = float (ld)

        summary = pd.DataFrame.from_dict (stats, orient='index')
        for m in ['spearman_ic', 'pearson_ir', 'pca_coeff']:
            arr = summary[m].to_numpy (dtype=float)
            if scaler == 'minmax':
                mi, ma = arr.min (), arr.max ()
                summary[f'{m}_norm'] = (summary[m] - mi) / (ma - mi) if ma != mi else 0.0
            else:
                mu, sd = arr.mean (), arr.std (ddof=0)
                summary[f'{m}_norm'] = (summary[m] - mu) / sd if sd != 0 else 0.0

        summary['combined_score'] = (
                summary['spearman_ic_norm']
                + summary['pearson_ir_norm']
                + summary['pca_coeff_norm']
        )
        summary = summary.sort_values ('combined_score', ascending=False)
        if top_k:  # Âè™‰øùÁïô top k ‰∏™Âõ†Â≠ê
            summary = summary.head (top_k)
            cols_to_keep = ['timestamp'] + summary.index.tolist ()
            self.df_features = self.df_features[cols_to_keep]
        self.summary = summary.round (self.decimals)
        return self.summary

    def get_summary (self, sort_by='combined_score', ascending=False) -> pd.DataFrame:
        return self.summary.sort_values (sort_by, ascending=ascending)

    def next (
            self,
            steps: int = 1,
            k: int = 1,
            mode: str = 'single',
            n_job: Optional[int] = None,
            bounded_only: bool = False
    ) -> pd.DataFrame:
        for _ in tqdm (range (steps), desc="üîÑ next steps"):
            features = list (self.summary.index)
            mat = self.df_features.set_index ('timestamp')[features].values
            corr = np.abs (np.corrcoef (mat.T))
            idx_map = {f: i for i, f in enumerate (features)}

            kept = [features.pop (0)]
            while len (kept) < k and features:
                max_corrs = [
                    max (corr[idx_map[f], idx_map[kf]] for kf in kept)
                    for f in features
                ]
                kept.append (features.pop (int (np.argmin (max_corrs))))

            sub_df = self.df_features.set_index ('timestamp')[kept]
            newf = self.cross_op (sub_df, mode, n_job, bounded_only)
            merged = pd.concat ([sub_df, newf], axis=1)
            drop_cols = merged.nunique ()[merged.nunique () <= 1].index.tolist ()
            merged = merged.drop (columns=drop_cols).dropna ()

            self.df_features = merged.reset_index ().round (self.decimals)
            self.evaluate_factors (**self._eval_kwargs)

        return self.df_features

    def visualize_structure_2d (
            self,
            seq_len: int,
            perplexity: float = 30.0,
            n_neighbors: int = 10,
            random_state: int = 42,
            pca_evp: Optional[float] = 0.90,
            umap_components: Optional[int] = 2,
            n_jobs: int = None
    ) -> None:
        """
        ‰∏ÄÊ¨°ÊÄßÁªòÂà∂ 6 Áßç‰∫åÁª¥ÈôçÁª¥Êï£ÁÇπÂõæÔºåÊåâ self.target_col ÁöÑÂâçÂêëÊ∂®Ë∑åÁùÄËâ≤„ÄÇ
        ÊîØÊåÅÂ§öÁ∫øÁ®ãÂπ∂Ë°åËÆ°ÁÆóÈôçÁª¥ÁªìÊûúÔºåËøõÂ∫¶Êù°Ë¶ÜÁõñÊªëÁ™óÂ±ïÂºÄÂíåÂπ∂Ë°åÈôçÁª¥Èò∂ÊÆµ„ÄÇ

        ÂèÇÊï∞Ôºö
          - seq_len: ‚Ä¶
          - perplexity: ‚Ä¶
          - n_neighbors: ‚Ä¶
          - random_state: ‚Ä¶
          - pca_evp:    PCA ‰øùÁïôÊñπÂ∑ÆÊØî‰æãÔºà0< pca_evp <=1ÔºâÔºåÊàñ None Ë∑≥ËøáËØ•ÊñπÊ°àÔºõ
          - umap_components: UMAP‚Üít-SNE È¶ñÊ≠•ËæìÂá∫Áª¥Â∫¶ÔºàÊï¥ÂûãÔºâÔºåÊàñ None Ë∑≥ËøáËØ•ÊñπÊ°àÔºõ
          - n_jobs:     Âπ∂Ë°åÁ∫øÁ®ãÊï∞‚Ä¶
        """

        import os
        from concurrent.futures import ThreadPoolExecutor, as_completed
        # ‚Äî‚Äî‚Äî 1. ÊèêÂèñÂ∑≤ÁîüÊàêÁöÑÁâπÂæÅÁü©Èòµ & Êó∂Èó¥Á¥¢Âºï ‚Äî‚Äî‚Äî
        df_feat = self.df_features.copy ()
        timestamps = df_feat['timestamp']
        X = df_feat.drop (columns=['timestamp']).values  # (T, d)
        T, d = X.shape

        # ‚Äî‚Äî‚Äî 2. ËÆ°ÁÆóÂâçÂêëÊî∂Áõä & ÂØπÈΩêÊ†áÁ≠æ ‚Äî‚Äî‚Äî
        fp = self._eval_kwargs['forward_period']
        forward_ret = (
                self.df_global[self.target_col].shift (-fp)
                / self.df_global[self.target_col]
                - 1
        )
        aligned_forward = forward_ret.reindex (timestamps).reset_index (drop=True)

        # ‚Äî‚Äî‚Äî 3. ÊªëÁ™óÂ±ïÂºÄ & Ê†áÂáÜÂåñÔºàÂ∏¶ËøõÂ∫¶Êù°Ôºâ ‚Äî‚Äî‚Äî
        N = T - seq_len + 1
        V = np.empty ((N, seq_len * d), dtype=float)
        for i in tqdm (range (N), desc="üîÑ Á™óÂè£Â±ïÂπ≥"):
            V[i, :] = X[i:i + seq_len, :].flatten ()
        V = (V - V.mean (axis=0)) / (V.std (axis=0, ddof=0) + 1e-8)

        # ‚Äî‚Äî‚Äî 4. ÁîüÊàêÊ∂®Ë∑åÊ†áÁ≠æ ‚Äî‚Äî‚Äî
        labels = (aligned_forward.iloc[seq_len - 1: seq_len - 1 + N] >= 0)
        labels = labels.astype (int).to_numpy ()

        # ‚Äî‚Äî‚Äî 5. ÂÆö‰πâÈôçÁª¥‰ªªÂä°ÂàóË°® ‚Äî‚Äî‚Äî
        reducers = [
            ('PCA-2', PCA (n_components=2, random_state=random_state)),
            ('LLE', LocallyLinearEmbedding (n_neighbors=n_neighbors, n_components=2, random_state=random_state)),
            ('t-SNE', TSNE (n_components=2, perplexity=perplexity, random_state=random_state)),
            (f'PCA({pca_evp * 100:.2f}%)‚Üít-SNE', pca_evp),
            ('UMAP-2', umap.UMAP (n_neighbors=n_neighbors, n_components=2)),
            (f'UMAP({umap_components})‚Üít-SNE', umap_components),
        ]
        results = {}
        # Âπ∂Ë°åËÆ°ÁÆóÈôçÁª¥
        n_workers = n_jobs or os.cpu_count ()

        def _compute (item):
            name, algo = item
            if name == f'PCA({pca_evp * 100:.2f}%)‚Üít-SNE':
                V_pca90 = PCA (n_components=algo, random_state=random_state).fit_transform (V)
                Z = TSNE (n_components=2, perplexity=perplexity, random_state=random_state).fit_transform (V_pca90)
            elif name == f'UMAP({umap_components})‚Üít-SNE':
                V_umap_components = umap.UMAP (
                    n_neighbors=n_neighbors,
                    n_components=algo,
                ).fit_transform (V)
                Z = TSNE (n_components=2, perplexity=perplexity, random_state=random_state).fit_transform (
                    V_umap_components)
            else:
                Z = algo.fit_transform (V)
            return name, Z

        with ThreadPoolExecutor (max_workers=n_workers) as executor:
            futures = {executor.submit (_compute, item): item for item in reducers}
            for fut in tqdm (as_completed (futures), total=len (futures), desc="üîÑ Âπ∂Ë°åÈôçÁª¥"):
                name, Z = fut.result ()
                results[name] = Z

        # ‚Äî‚Äî‚Äî 6. ÁªòÂõæÔºà2x3 ÁΩëÊ†ºÔºå‰ΩøÁî® constrained_layoutÔºâ ‚Äî‚Äî‚Äî
        fig, axes = plt.subplots (2, 3, figsize=(18, 10), constrained_layout=True)
        cm = plt.cm.coolwarm

        for ax, (title, Z) in zip (axes.flatten (), results.items ()):
            sc = ax.scatter (Z[:, 0], Z[:, 1], c=labels, cmap=cm, s=10, alpha=0.7)
            ax.set_title (title, fontsize=12)
            ax.set_xlabel ('Component 1')
            ax.set_ylabel ('Component 2')

        cbar = fig.colorbar (sc, ax=axes.ravel ().tolist (), ticks=[0, 1])
        cbar.ax.set_yticklabels (['Down', 'Up'])

        fig.suptitle (f'2D Structure Visualization (seq_len={seq_len})', fontsize=14)
        plt.show ()

    def evaluate_clusterings (
            self,
            algos: Optional[list] = None,
            param_grids: Optional[Dict[str, Dict[str, list]]] = None,
            metrics: Optional[list] = None,
            dim_reduction: str = 'none',
            reduction_params: Optional[Dict[str, Any]] = None,
            seq_len: int = 1,
            n_jobs: int = 1,
            backend: str = 'thread',
    ) -> pd.DataFrame:
        """
        ÂØπÂ§öÁßçËÅöÁ±ªÁÆóÊ≥ïÂíåÂèÇÊï∞ÁªÑÂêàËøõË°åËØÑ‰º∞ÔºåÂπ∂Â∞ÜÊúÄ‰Ω≥ËÅöÁ±ªÊ†áÁ≠æÂ≠òÂÖ• self.df_features['cluster']„ÄÇ

        ÂèÇÊï∞:
          - algos: Optional[Dict[str, Estimator]]ÔºåÁÆóÊ≥ïÊò†Â∞ÑÔºåÈîÆ‰∏∫ÁÆóÊ≥ïÂêçÁß∞Â≠óÁ¨¶‰∏≤Ôºå
            ÂÄº‰∏∫ËÅöÁ±ªÂô®Á±ªÔºõÊîØÊåÅÁöÑÁÆóÊ≥ïÂêçÁß∞ÂåÖÊã¨Ôºö
            'KMeans', 'AgglomerativeClustering', 'SpectralClustering',
            'GaussianMixture', 'DBSCAN', 'MeanShift'„ÄÇ
            None Ë°®Á§∫‰ΩøÁî®ÊâÄÊúâÈªòËÆ§ÁÆóÊ≥ï„ÄÇ

          - param_grids: Optional[Dict[str, Dict[str, list]]]ÔºåË∂ÖÂèÇÊï∞ÁΩëÊ†ºÔºå
            ÈîÆ‰∏é algos ‰∏≠ÁÆóÊ≥ïÂêçÁß∞ÂØπÂ∫îÔºåÂÄº‰∏∫ {ÂèÇÊï∞Âêç: ÂÄôÈÄâÂÄºÂàóË°®}„ÄÇ
            None Ë°®Á§∫‰ΩøÁî®ÈªòËÆ§ÁΩëÊ†ºÔºö
              ‚Ä¢ KMeans: {'n_clusters': [2..10], 'random_state': [0]}
              ‚Ä¢ AgglomerativeClustering: {'n_clusters': [2..10], 'linkage': ['ward','average','complete']}
              ‚Ä¢ SpectralClustering: {'n_clusters': [2..10], 'affinity': ['rbf','nearest_neighbors'], 'random_state': [0]}
              ‚Ä¢ GaussianMixture: {'n_components': [2..10], 'covariance_type': ['full'], 'random_state': [0]}
              ‚Ä¢ DBSCAN: {'eps': [0.1,0.2,0.5], 'min_samples': [5,10,20]}
              ‚Ä¢ MeanShift: {'bandwidth': ['estimate']} (Ëá™Âä®‰º∞ÁÆóÂ∏¶ÂÆΩ)

          - metrics: Optional[List[str]]ÔºåËØÑ‰º∞ÊåáÊ†áÂàóË°®ÔºåÂèØÈÄâÔºö
            'silhouette', 'calinski_harabasz', 'davies_bouldin'„ÄÇ
            None Êó∂‰ΩøÁî®‰∏âËÄÖÂÖ®ÈÉ®„ÄÇ

          - dim_reduction: strÔºåÈôçÁª¥ÊñπÂºèÔºå
            'none'ÔºàÈªòËÆ§Ôºå‰∏çÈôçÁª¥Ôºâ„ÄÅ
            'pca'ÔºàPCA ‰øùÁïôÊñπÂ∑ÆÊØî‰æãÔºâ„ÄÅ
            'umap'ÔºàUMAP ÈôçÁª¥Ôºâ„ÄÇ

          - reduction_params: Optional[Dict[str, Any]]ÔºåÈôçÁª¥ÂèÇÊï∞Ôºõ
            ÂΩì dim_reduction='pca' Êó∂ÔºåÊîØÊåÅÔºö
              {'variance_ratio': float (0<var<=1)}Ôºõ
            ÂΩì dim_reduction='umap' Êó∂ÔºåÊîØÊåÅÔºö
              {'n_components': int, 'n_neighbors': int, 'min_dist': float, ...}„ÄÇ
            None ‰ΩøÁî®ÁÆóÊ≥ïÈªòËÆ§ÂèÇÊï∞„ÄÇ

          - seq_len: intÔºåÂ∫èÂàóÁ™óÂè£ÈïøÂ∫¶Ôºå
            Â§ß‰∫é 1 Êó∂Â∞ÜÂéüÂßã (T, d) Êï∞ÊçÆÊåâÁ™óÂè£Â±ïÂºÄ‰∏∫
            (T-seq_len+1, seq_len*d) Ê†∑Êú¨ÔºåÊ†áÁ≠æÂØπÂ∫îÁ™óÂè£Êú´Â∞æ timestamp„ÄÇ

          - n_jobs: intÔºåÂπ∂Ë°å worker Êï∞ÈáèÔºå>0„ÄÇ
            backend='thread' Êó∂ÂêØÂä®Á∫øÁ®ãÊ±†Ôºå
            backend='process' Êó∂ÂêØÂä®ËøõÁ®ãÊ±†„ÄÇ

          - backend: strÔºåÂπ∂Ë°åÁ±ªÂûãÔºå'thread' Êàñ 'process'„ÄÇ
            'thread' ÈÄÇÂêàÈáäÊîæ GIL ÁöÑ sklearn ÁÆóÊ≥ïÔºå
            'process' ÈÄÇÂêà CPU ÂØÜÈõÜÂûã„ÄÅÁ∫Ø Python ‰ªªÂä°„ÄÇ
        ËøîÂõû:
          - pd.DataFrameÔºöÊØèË°åÂØπÂ∫î‰∏ÄÊ¨°ÁÆóÊ≥ï+ÂèÇÊï∞ÁªÑÂêàÁöÑËØÑ‰º∞Ôºå
            ÂàóÂåÖÊã¨ 'algo', ÂêÑË∂ÖÂèÇÊï∞, 'silhouette', 'calinski_harabasz', 'davies_bouldin'„ÄÇ
        """

        # ÈªòËÆ§ÁÆóÊ≥ï‰∏éÂèÇÊï∞ÁΩëÊ†º
        default_algos = {
            'KMeans': KMeans,
            'AgglomerativeClustering': AgglomerativeClustering,
            'SpectralClustering': SpectralClustering,
            'GaussianMixture': GaussianMixture,
            'DBSCAN': DBSCAN,
            'MeanShift': MeanShift,
        }
        default_param_grids = {
            'KMeans': {'n_clusters': list (range (2, 11)), 'random_state': [0]},
            'AgglomerativeClustering': {'n_clusters': list (range (2, 11)), 'linkage': ['ward', 'average', 'complete']},
            'SpectralClustering': {'n_clusters': list (range (2, 11)), 'affinity': ['rbf', 'nearest_neighbors'],
                                   'random_state': [0]},
            'GaussianMixture': {'n_components': list (range (2, 11)), 'covariance_type': ['full'], 'random_state': [0]},
            'DBSCAN': {'eps': [0.1, 0.2, 0.5], 'min_samples': [5, 10, 20]},
            'MeanShift': {'bandwidth': ['estimate']}
        }
        # Ëß£Êûê algos
        if algos is None:
            to_run = default_algos
        elif isinstance (algos, list):
            to_run = {name: default_algos[name] for name in algos if name in default_algos}
        else:
            raise ValueError ("`algos` must be None or list of algorithm names")
        # Ëß£Êûê param_grids
        if param_grids is None:
            grids = default_param_grids
        else:
            grids = {name: param_grids.get (name, default_param_grids[name]) for name in to_run}
        metrics = metrics or ['silhouette', 'calinski_harabasz', 'davies_bouldin']

        # ÊûÑÈÄ† X ÂíåÊó∂Èó¥Á¥¢Âºï
        df = self.df_features.drop (columns=['cluster'], errors='ignore').copy ()
        # ‰ªÖ‰øùÁïôÂõ†Â≠êÁâπÂæÅÂàóÔºåÊéíÈô§ timestamp Âíå clusterÔºàÈò≤Ê≠¢ NaN ÂºïÂÖ•Ôºâ
        feature_cols = [c for c in df.columns if c not in ('timestamp', 'cluster')]
        X0 = df[feature_cols].values
        times = df['timestamp'].tolist ()
        if seq_len > 1:
            T, d = X0.shape
            N = T - seq_len + 1
            X = np.vstack ([X0[i:i + seq_len].flatten () for i in range (N)])
            time_index = [times[i + seq_len - 1] for i in range (N)]
        else:
            X = X0.copy ()
            time_index = times

        # Ê†áÂáÜÂåñ
        scaler = (MinMaxScaler () if self._eval_kwargs['scaler'] == 'minmax'
                  else RobustScaler () if self._eval_kwargs['scaler'] == 'robust'
        else StandardScaler ())

        X = scaler.fit_transform (X)

        # ÈôçÁª¥
        reduction_params = reduction_params or {}
        if dim_reduction == 'pca':
            vr = reduction_params.get ('variance_ratio', 1.0)
            pca = PCA (n_components=vr, svd_solver='full', random_state=0)
            X = pca.fit_transform (X)
        elif dim_reduction == 'umap':
            n_c = reduction_params.get ('n_components', 2)
            umap_kwargs = {k: v for k, v in reduction_params.items () if k != 'n_components'}
            reducer = umap.UMAP (n_components=n_c, **umap_kwargs)
            X = reducer.fit_transform (X)

        # Âπ∂Ë°åËØÑ‰º∞
        Executor = ThreadPoolExecutor if backend == 'thread' else ProcessPoolExecutor
        tasks = [(name, Est, params, X, metrics)
                 for name, Est in to_run.items ()
                 for params in ParameterGrid (grids.get (name, {}))]
        records = []
        with Executor (max_workers=n_jobs) as executor:
            futures = {executor.submit (_evaluate_clustering_task, *task): task for task in tasks}
            iterator = as_completed (futures)
            iterator = tqdm (iterator, total=len (futures), desc='Clustering eval')
            for fut in iterator:
                rec = fut.result ()
                if rec:
                    records.append (rec)

        df_eval = pd.DataFrame.from_records (records)
        self.cluster_eval_ = df_eval.drop (columns=['_labels'], errors='ignore')

        # ÁªºÂêàÁªòÂõæ
        plt.figure ()
        best_sil = df_eval.groupby ('algo')['silhouette'].max ()
        best_sil.plot (kind='bar')
        plt.ylabel ('Max Silhouette')
        plt.title (f'Algorithm Comparison (dim_red={dim_reduction})')
        plt.show ()

        # silhouette plot
        best_idxs = df_eval.groupby ('algo')['silhouette'].idxmax ().items ()
        items = list (best_idxs)

        # ÊØè 3 ‰∏™ÁÆóÊ≥ïÂÅö‰∏ÄÁªÑ
        for i in range (0, len (items), 3):
            group = items[i:i + 3]
            n = len (group)
            # ÊØèË°å n ÂàóÂ≠êÂõæ
            fig, axes = plt.subplots (1, n, figsize=(5 * n, 4), squeeze=False)
            for ax, (algo, idx) in zip (axes[0], group):
                labels = df_eval.loc[idx, '_labels']
                # ÂÆâÂÖ®Ëé∑ÂèñÁ∞áÊï∞
                raw_nc = df_eval.loc[idx, 'n_clusters'] if 'n_clusters' in df_eval.columns else np.nan
                if pd.notna (raw_nc):
                    n_clusters = int (raw_nc)
                else:
                    unique_labels = np.unique (labels)
                    n_clusters = int ((unique_labels != -1).sum ())
                if n_clusters < 2:
                    ax.text (0.5, 0.5, f"{algo}\nÂè™Êúâ{n_clusters}Á∞áÔºåË∑≥Ëøá",
                             ha='center', va='center')
                    ax.axis ('off')
                    continue

                sil_vals = silhouette_samples (X, labels)
                y_lower = 10
                for j in range (n_clusters):
                    ith = np.sort (sil_vals[labels == j])
                    size_j = ith.shape[0]
                    y_upper = y_lower + size_j
                    ax.fill_betweenx (
                        np.arange (y_lower, y_upper),
                        0, ith, alpha=0.7
                    )
                    y_lower = y_upper + 10
                ax.set_title (f"{algo} (clusters={n_clusters})")
                ax.set_xlabel ("Silhouette")
                ax.set_ylabel ("Cluster label")
                ax.set_yticks ([])
                ax.axvline (df_eval.loc[idx, 'silhouette'], color='red', linestyle='--')

            plt.tight_layout ()
            plt.show ()

            self.cluster_report = df_eval

        return df_eval
