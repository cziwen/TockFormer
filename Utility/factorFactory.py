import time

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import umap
import warnings
import os
import gc
import shutil

from typing import List, Optional, Dict, Any
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed, wait, FIRST_COMPLETED

from tqdm import tqdm
from scipy.stats import spearmanr
from sklearn.decomposition import PCA, IncrementalPCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.model_selection import ParameterGrid
from sklearn.metrics import (
    silhouette_score,
    calinski_harabasz_score,
    davies_bouldin_score,
    silhouette_samples
)
from sklearn.exceptions import ConvergenceWarning
from sklearn.cluster import (
    KMeans,
    AgglomerativeClustering,
    SpectralClustering,
    DBSCAN,
    MeanShift,
    estimate_bandwidth
)
from sklearn.mixture import GaussianMixture

from Utility.factors import *
from Utility.IOcache import dump_dataframe, load_dataframe, wash, delete_parquet_files


# ================ Pickle Function ================

def _evaluate_factor_task_streaming(cache_dir: str, fname: str, win_eff: int, fr:pd.Series):
    """
    Module-level helper to compute Spearman IC and Pearson IR for a single factor column.
    """
    path = os.path.join(cache_dir, fname)
    df = pd.read_parquet(path)
    col = df.columns[0]
    series = df[col]

    # Align and drop NaNs
    series = series.reindex(fr.index)
    sub = pd.DataFrame({col: series, 'forward_return': fr}).dropna()

    # Spearman IC
    raw_sp = spearmanr(sub[col], sub['forward_return']).correlation
    sp = float(np.atleast_1d(raw_sp).ravel()[0])

    # Rolling IR
    roll = sub[col].rolling(win_eff).corr(sub['forward_return']).dropna().values
    if roll.size == 0 or np.nanstd(roll, ddof=0) == 0:
        ir = np.nan
    else:
        ir = float(np.nanmean(roll) / np.nanstd(roll, ddof=0))

    return col, sp, ir


def _compute_norm (metric: str, arr: np.ndarray, scaler: str):
    """
    è¿”å› (metric, norm_arr)ï¼Œå¯¹å…¨ NaN æˆ–å¸¸æ•°æƒ…å†µåšä¿æŠ¤ã€‚
    """
    # å…ˆæ£€æµ‹æ•´åˆ—æ˜¯å¦å…¨æ˜¯ NaN
    if np.all (np.isnan (arr)):
        return metric, np.zeros_like (arr)

    if scaler == 'minmax':
        # è·³è¿‡ NaNï¼Œè®¡ç®— min/max
        mi = np.nanmin (arr)
        ma = np.nanmax (arr)
        # å¦‚æœæœ€å¤§æœ€å°ç›¸ç­‰ï¼Œæˆ–éƒ½ä¸º NaNï¼ˆnanmin/nanmax å·²è·³è¿‡ï¼‰ï¼Œå¡« 0
        if ma == mi:
            norm = np.zeros_like (arr)
        else:
            norm = (arr - mi) / (ma - mi)
    else:
        # è·³è¿‡ NaNï¼Œè®¡ç®— mean/std
        mu = np.nanmean (arr)
        sd = np.nanstd (arr, ddof=0)
        # å¦‚æœ std ä¸º 0ï¼Œå¡« 0
        if sd == 0:
            norm = np.zeros_like (arr)
        else:
            norm = (arr - mu) / sd

    # æœ€åæŠŠä»»ä½•å¯èƒ½æ®‹ç•™çš„ NaNï¼ˆç†è®ºä¸Šä¸åº”è¯¥æœ‰ï¼‰ä¹Ÿå¡«æˆ 0
    norm = np.where (np.isnan (norm), 0.0, norm)
    return metric, norm


def _compute_one (df_sub: pd.DataFrame, col: str, win_eff: int):
    raw_sp = spearmanr (df_sub[col], df_sub['forward_return']).correlation
    sp = float (np.atleast_1d (raw_sp).ravel ()[0])

    roll = df_sub[col].rolling (win_eff).corr (df_sub['forward_return']).dropna ().values
    # tqdm.write(f"roll.shape = {roll.shape}")

    if roll.size == 0:
        return col, sp, np.nan

    # å†æ£€æŸ¥ä¸€ä¸‹ï¼šå¦‚æœå…¨ NaNï¼ˆdropna åä¸å¯èƒ½ï¼‰ï¼Œæˆ–æ‰€æœ‰å€¼éƒ½ä¸€æ ·ï¼Œç›´æ¥ NaN
    if np.nanstd (roll, ddof=0) == 0:
        return col, sp, np.nan

    # ç”¨ nanâ€safe ç‰ˆæœ¬
    mean = np.nanmean (roll)
    std = np.nanstd (roll, ddof=0)
    ir = float (mean / std) if std > 0 else np.nan
    return col, sp, ir


def _cross_apply (args):
    op, a, b, arr_a, arr_b = args
    name = f"{a}_{op['name']}_{b}"
    return name, op['func'] (arr_a, arr_b)


def _evaluate_clustering_task (algo_name, Est, params, X, metrics):
    """
    é¡¶çº§å‡½æ•°ï¼šè¯„ä¼°å•ä¸ªç®—æ³•+å‚æ•°ç»„åˆã€‚
    è¿”å› dict æˆ– Noneã€‚
    """
    # é™åˆ¶ç°‡æ•°ä¸è¶…è¿‡æ ·æœ¬æ•°-1
    if 'n_clusters' in params and params['n_clusters'] > X.shape[0] - 1:
        return None
    try:
        with warnings.catch_warnings ():
            warnings.simplefilter ('error', UserWarning)
            warnings.simplefilter ('error', ConvergenceWarning)
            if Est is MeanShift and params.get ('bandwidth') == 'estimate':
                bw = estimate_bandwidth (X)
                model = Est (bandwidth=bw)
            else:
                model = Est (**params)
            labels = model.fit_predict (X) if hasattr (model, 'fit_predict') else model.fit (X).predict (X)
        if len (set (labels)) < 2:
            return None
        rec = {'algo': algo_name}
        rec.update (params)
        if 'silhouette' in metrics:
            rec['silhouette'] = silhouette_score (X, labels)
        if 'calinski_harabasz' in metrics:
            rec['calinski_harabasz'] = calinski_harabasz_score (X, labels)
        if 'davies_bouldin' in metrics:
            rec['davies_bouldin'] = davies_bouldin_score (X, labels)
        rec['_labels'] = labels
        return rec
    except (UserWarning, ConvergenceWarning):
        return None


class FactorFactory:
    def __init__ (
            self,
            df: pd.DataFrame,
            base_cols: Optional[List[str]] = None,
            target_col: str = 'close',
            forward_period: int = 1,
            window: int = 20,
            scaler: str = 'minmax',
            top_k: Optional[int] = None,
            decimals: int = 6,
            n_jobs: Optional[int] = None,
            use_disk_cache: bool = True,
            cache_dir: str = './tmp',
    ):
        # === å¤šçº¿ç¨‹é…ç½® ===
        if n_jobs is None or n_jobs < 1:
            n_jobs = os.cpu_count ()
        self.n_jobs = n_jobs

        # === ç£ç›˜ç¼“å­˜é…ç½® ===
        self.use_disk_cache = use_disk_cache
        self.cache_dir = cache_dir


        # åŸå§‹æ•°æ®æŒ‰ timestamp ç´¢å¼•
        self.df_global = (
            df.copy ()
            .sort_values ('timestamp')
            .set_index ('timestamp')
        )
        self.df_global_path = f'{cache_dir}/df_global'
        dump_dataframe(self.df_global, self.df_global_path, clear=True) # ç£ç›˜åŒ–

        self.base_cols = (
            base_cols if base_cols is not None
            else [c for c in self.df_global.select_dtypes (include=[np.number]).columns]
        )
        self.target_col = target_col
        self._eval_kwargs = dict (
            forward_period=forward_period,
            window=window,
            scaler=scaler,
            top_k=top_k
        )
        self.decimals = decimals

        self.df_features_path = f'{cache_dir}/df_features'
        self.df_features = pd.DataFrame ()
        dump_dataframe(self.df_features, self.df_features_path, clear=True)

        self.summary = pd.DataFrame ()

        self.cluster_report = pd.DataFrame ()

        del self.df_global

    def apply_registry (
            self,
            df: pd.DataFrame,
            cols: List[str],
            bounded_only: bool = False,
    ) -> pd.DataFrame:
        """
        å¯¹ä¼ å…¥çš„ dfï¼ˆå¿…é¡»æœ‰ timestamp ç´¢å¼•ï¼‰ï¼ŒæŒ‰åˆ—å¹¶è¡Œæ‰§è¡Œ FACTOR_REGISTRY ä¸­çš„ funcï¼Œ
        æ¯ä¸ªçº¿ç¨‹åªè´Ÿè´£ä¸€ä¸ª (prefix, col) ç»„åˆã€‚
        """
        # # å¤‡ä»½åŸå±æ€§ ï¼ˆå–æ¶ˆï¼‰
        # orig_df, orig_base, orig_target = self.df_global, self.base_cols, self.target_col
        #
        # # ä¸´æ—¶æ›¿æ¢
        # self.df_global = df.copy ()
        # self.base_cols = cols.copy ()
        # self.target_col = self.target_col if self.target_col in df.columns else None

        new_feats: Dict[str, pd.Series] = {}
        tasks = []
        for prefix, info in FACTOR_REGISTRY.items ():
            tags = info.get ('category', [])
            if not isinstance (tags, list):
                tags = [tags]
            if bounded_only and 'bounded' not in tags:
                continue
            # å¯¹æ¯ä¸ª col å•ç‹¬æäº¤ä¸€ä¸ªä»»åŠ¡
            for col in cols:
                tasks.append ((prefix, info['func'], col))

        # å¤šè¿›ç¨‹æ‰§è¡Œ
        with ProcessPoolExecutor (max_workers=self.n_jobs) as executor:
            future_to_task = {
                executor.submit (func, df[[col]], [col]): (prefix, col)
                for prefix, func, col in tasks
            }
            for fut in tqdm (as_completed (future_to_task), total=len (future_to_task), desc='Applying Factors'):
                out = fut.result ()
                # å°†ç»“æœæ”¶é›†åˆ° new_feats
                for key, series in out.items ():
                    new_feats[key] = pd.Series (series, index=df.index, name=key)

        # æ¢å¤åŸå±æ€§ (å–æ¶ˆ)
        # self.df_global, self.base_cols, self.target_col = orig_df, orig_base, orig_target
        reg_df = pd.DataFrame (new_feats, index=df.index)

        # if self.use_disk_cache and not on_memory:
            # def _write_parquet (name_series): # ï¼ˆå–æ¶ˆï¼‰
            #     name, series = name_series
            #     path = os.path.join (self.cache_dir, f"{name}.parquet")
            #     series.to_frame (name).to_parquet (path)
            #
            # # å‡†å¤‡æ‰€æœ‰è¦å†™å…¥çš„ (name, series)
            # items = list (reg_df.items ())
            # # å¹¶è¡Œå†™ç›˜
            # with ThreadPoolExecutor (max_workers=self.n_jobs) as executor:
            #     futures = [executor.submit (_write_parquet, item) for item in items]
            #     for fut in tqdm (as_completed (futures), total=len (futures), desc='IO'):
            #         fut.result ()  # æŠ›å‡ºå¯èƒ½çš„å¼‚å¸¸
            # return pd.DataFrame ()

        return reg_df

    def generate_factors (self, mode: str = 'single', bounded_only: bool = False):
        """
        Generate and evaluate uni- and cross-features, with optional disk caching.
        """

        # è¯»å–åŸå§‹df
        self.df_global = load_dataframe(self.df_global_path)

        # Compute cross features
        self._get_cross_features (self.df_global, mode, bounded_only)

        del self.df_global


        # clean
        # df_feat = self._merge_clean_round ([cross_df])
        wash(self.df_features_path)


        # 3) Store and evaluate
        # self.df_features = df_feat.reset_index ()
        self.evaluate_factors (**self._eval_kwargs)
        # return self.df_features

    def _get_cross_features (self, feat_df: pd.DataFrame, mode: str, bounded_only: bool) -> pd.DataFrame:
        """
        Compute second-order (cross) features, optionally using disk cache.
        """
        if self.use_disk_cache:
            # Compute and cache merged features
            df_all = self.cross_op (feat_df, mode, bounded_only)
            return df_all

        # In-memory cross features
        return self.cross_op (feat_df, mode, bounded_only)

    def cross_op (
            self,
            input_df: pd.DataFrame,
            mode: str = 'single',
            bounded_only: bool = False
    ) -> pd.DataFrame:
        data = input_df.copy ()
        feats: Dict[str, pd.Series] = {}
        file_map: Dict[str, str] = {}

        # â€”â€” ä¸€é˜¶å› å­ â€”â€”
        reg_df = self.apply_registry (
            df=data,
            cols=list (data.columns),
            bounded_only=bounded_only,
        )

        if self.use_disk_cache:
            dump_dataframe(reg_df, self.df_features_path) # ç£ç›˜åŒ–
            del reg_df
        else:
            feats.update (reg_df.to_dict ('series'))

        # â€”â€” ä¸€å…ƒç®—å­ â€”â€”
        unary_tasks = []
        for col in data.columns:
            for op in CROSS_OPS:
                if op['arity'] == 1 and (not bounded_only or op['preserves_bounded']):
                    name = f"{op['name']}_({col})"
                    func = op['func']
                    arr = data[col].values
                    unary_tasks.append ((name, func, arr))

        def _process_unary (task):
            name, func, arr = task
            series = pd.Series (func (arr), name=name)
            series.index = data.index
            return name, series

        compute_pool = ThreadPoolExecutor (max_workers=self.n_jobs)

        compute_futs = {compute_pool.submit (_process_unary, t): t for t in unary_tasks}

        unary_futures: Dict[str, pd.Series] = {}
        for fut in tqdm(as_completed(compute_futs), total=len (compute_futs), desc='ğŸ”„ Unary op'):
            name, series = fut.result ()
            unary_futures[name] = series

        dump_dataframe(pd.DataFrame(unary_futures, index=data.index), self.df_features_path)
        del unary_futures

        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹æ± ä»»åŠ¡å®Œæˆ (å–æ¶ˆ)
        compute_pool.shutdown (wait=True)
        gc.collect ()



        # â€”â€” äºŒå…ƒç®—å­æ„é€ ä»»åŠ¡åˆ—è¡¨ â€”â€”
        dd = data.to_dict ('series')
        tasks = [
            (op, a, b, dd[a].values, dd[b].values)
            for op in CROSS_OPS
            if op['arity'] == 2 and (not bounded_only or op['preserves_bounded'])
            for a in data.columns for b in data.columns
        ]

        if mode == 'thread':
            batch_size = (self.n_jobs or os.cpu_count ()) * 4
            idx = 0
            total = len (tasks)
            pbar = tqdm (total=total, desc="ğŸ”„ cross op (thread)")
            pending = set ()
            io_futures = set ()

            # ç”¨äºæ‰¹é‡å†™ç›˜çš„ç¼“å­˜å’Œçº¿ç¨‹æ± 
            io_pool = ThreadPoolExecutor (max_workers=self.n_jobs)
            with ThreadPoolExecutor (max_workers=self.n_jobs) as executor:
                # åˆå§‹æäº¤ batch_size ä¸ªè®¡ç®—ä»»åŠ¡
                while idx < total and len (pending) < batch_size:
                    pending.add (executor.submit (_cross_apply, tasks[idx]))
                    idx += 1

                # å¾ªç¯ï¼Œç›´åˆ°æ‰€æœ‰è®¡ç®—ä»»åŠ¡å®Œæˆ
                while pending:
                    done, pending = wait (pending, return_when=FIRST_COMPLETED)
                    for fut in done:
                        name, series = fut.result ()
                        series.index = data.index
                        if self.use_disk_cache:
                            path = os.path.join (self.df_features_path, f"{name}.parquet")
                            # æäº¤å†™ç›˜
                            io_futures.add (io_pool.submit (
                                lambda s=series, p=path, n=name: s.to_frame (n).to_parquet (p)
                            ))
                            file_map[name] = path
                            del series
                        else:
                            feats[name] = series
                        pbar.update (1)
                        # ä¿æŒæ»‘åŠ¨çª—å£å¤§å°
                        if idx < total:
                            pending.add (executor.submit (_cross_apply, tasks[idx]))
                            idx += 1

                        # å¦‚æœå†™ç›˜é˜Ÿåˆ—è¿‡é•¿ï¼Œå°±ç­‰æœ€å…ˆå®Œæˆçš„ä¸€æ‰¹å›æ¥
                        if len (io_futures) >= batch_size:
                            done_io, io_futures = wait (io_futures, return_when=FIRST_COMPLETED)

                        # å¦‚æœè¾¾åˆ°ä¸€ä¸ªæ‰¹æ¬¡ï¼Œæˆ–è®¡ç®—å…¨å®Œæˆï¼Œå›æ”¶åƒåœ¾
                        if idx % batch_size == 0 or not pending:
                            gc.collect ()
            pbar.close ()
            io_pool.shutdown (wait=True)


        else:  # single
            for t in tqdm (tasks, total=len (tasks), desc="ğŸ”„ cross_op (single)"):
                name, series = _cross_apply (t)
                feats[name] = series


        # === å¦‚æœå¼€å¯ç£ç›˜ç¼“å­˜ï¼Œæœ€åä¸€æ¬¡æ€§åˆå¹¶æ‰€æœ‰ parquet æ–‡ä»¶ ===
        if self.use_disk_cache:
            # # æ‰¾å‡ºæ‰€æœ‰ parquet æ–‡ä»¶è·¯å¾„ (å–æ¶ˆ)
            # paths = [
            #     os.path.join (self.cache_dir, f)
            #     for f in os.listdir (self.cache_dir)
            #     if f.endswith ('.parquet')
            # ]
            # # å¹¶è¡Œè¯»å– parquet
            # df_list = []
            # with ThreadPoolExecutor (max_workers=self.n_jobs) as executor:
            #     futures = {executor.submit (pd.read_parquet, p): p for p in paths}
            #     for fut in tqdm (as_completed (futures), total=len (paths), desc="IO"):
            #         df_list.append (fut.result ())
            # # åˆå¹¶å¹¶æ¸…ç†ç¼“å­˜ç›®å½•
            # merged = pd.concat (df_list, axis=1)
            # shutil.rmtree (self.cache_dir)

            # merged = load_dataframe(self.df_features_path)
            # return merged
            return pd.DataFrame () # å ä½ç¬¦

        df_new = pd.DataFrame (feats).reindex (data.index)
        df_new.index.name = 'timestamp'
        return df_new


    def evaluate_factors (
            self,
            forward_period: int,
            window: int,
            scaler: str = 'minmax',
            top_k: Optional[int] = None,
    ) -> pd.DataFrame:
        """
        å†…å­˜å‹å¥½ç‰ˆ evaluate_factorsï¼šæŒ‰éœ€åŠ è½½æ¯åˆ— Parquetï¼Œ
        å¹¶ç”¨ IncrementalPCA åˆ†æ‰¹æ¬¡åš PCAã€‚
        """
        # 1) åªåŠ è½½ä¸€æ¬¡ï¼šç›®æ ‡ä»·æ ¼åˆ—ï¼Œè®¡ç®—å‰å‘æ”¶ç›Š
        price_df = load_dataframe (
            cache_dir=self.df_global_path,  # å­˜æ”¾ df_global çš„ç›®å½•
            n_jobs=self.n_jobs,
            columns=[self.target_col]
        )
        price = price_df[self.target_col]
        del price_df
        returns = (price.shift (-forward_period) / price - 1).dropna ()
        win_eff = min (window, max (1, len (returns) - 1))

        # 2) å¹¶è¡Œè®¡ç®— Spearman IC & IR
        factor_files = [
            f for f in os.listdir (self.df_features_path)
            if f.endswith ('.parquet')
        ]


        stats: Dict[str, Dict[str, float]] = {}
        with ProcessPoolExecutor (max_workers=self.n_jobs) as exe:
            futures = {exe.submit (_evaluate_factor_task_streaming, self.df_features_path,fn, win_eff, returns): fn for fn in factor_files}
            for fut in tqdm(as_completed (futures), total=len(futures), desc='ic eval'):
                col, sp, ir = fut.result ()
                stats[col] = {'spearman_ic': sp, 'pearson_ir': ir}

        # 3) PCA æå–ä¸€ä¸»æˆåˆ†æƒé‡
        factor_names = list (stats.keys ())
        # ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰å› å­ç‰¹å¾çŸ©é˜µ
        df_pca = load_dataframe (
            cache_dir=self.df_features_path,
            n_jobs=self.n_jobs,
            columns=factor_names
        )
        X = df_pca.to_numpy (dtype=float)
        std_scaler = StandardScaler ()
        X_scaled = std_scaler.fit_transform (X)
        # æ ‡å‡† PCA
        pca = PCA (n_components=1)
        pca.fit (X_scaled)
        coeffs = np.abs (pca.components_[0])
        for f, w in zip (factor_names, coeffs):
            stats[f]['pca_coeff'] = float (w)


        # 4) æ„å»º summary è¡¨å¹¶å½’ä¸€åŒ–
        summary = pd.DataFrame.from_dict (stats, orient='index')
        for m in ['spearman_ic', 'pearson_ir', 'pca_coeff']:
            _, norm_arr = _compute_norm (m, summary[m].to_numpy (dtype=float), scaler)
            summary[f'{m}_norm'] = norm_arr

        summary['combined_score'] = (
                summary['spearman_ic_norm']
                + summary['pearson_ir_norm']
                + summary['pca_coeff_norm']
        )
        summary = summary.sort_values ('combined_score', ascending=False).round (self.decimals)

        # 5) ä¿ç•™ top_k
        if top_k:
            retained = summary.head (top_k).index.tolist ()
            # åˆ é™¤æœªä¿ç•™çš„ Parquet æ–‡ä»¶
            to_delete = [col for col in stats.keys () if col not in retained]
            delete_parquet_files (self.df_features_path, to_delete)
            summary = summary.loc[retained]


        # å­˜å›å¯¹è±¡å¹¶è¿”å›
        self.summary = summary
        return summary

    def get_summary (self, sort_by='combined_score', ascending=False) -> pd.DataFrame:
        return self.summary.sort_values (sort_by, ascending=ascending)

    def next (
            self,
            k: int = 1,
            mode: str = 'single',
            bounded_only: bool = False
    ):

        self.df_features = load_dataframe (self.df_features_path)
        features = list (self.summary.index)
        mat = self.df_features[features].values
        corr = np.abs (np.corrcoef (mat.T))
        idx_map = {f: i for i, f in enumerate (features)}

        kept = [features.pop (0)]
        while len (kept) < k and features: # åœ¨ top-k é‡Œè´ªå¿ƒé€‰ k ä¸ªå› å­
            max_corrs = [
                max (corr[idx_map[f], idx_map[kf]] for kf in kept)
                for f in features
            ]
            kept.append (features.pop (int (np.argmin (max_corrs))))

        sub_df = self.df_features[kept]
        self.cross_op (sub_df, mode, bounded_only)
        del sub_df; del self.df_features
        wash(self.df_features_path)
        self.df_features = load_dataframe (self.df_features_path)

        self.evaluate_factors (**self._eval_kwargs)

    def visualize_structure_2d (
            self,
            seq_len: int,
            perplexity: float = 30.0,
            n_neighbors: int = 10,
            random_state: int = 42,
            pca_evp: Optional[float] = 0.90,
            umap_components: Optional[int] = 2,
    ) -> None:
        """
        ä¸€æ¬¡æ€§ç»˜åˆ¶ 6 ç§äºŒç»´é™ç»´æ•£ç‚¹å›¾ï¼ŒæŒ‰ self.target_col çš„å‰å‘æ¶¨è·Œç€è‰²ã€‚
        æ”¯æŒå¤šçº¿ç¨‹å¹¶è¡Œè®¡ç®—é™ç»´ç»“æœï¼Œè¿›åº¦æ¡è¦†ç›–æ»‘çª—å±•å¼€å’Œå¹¶è¡Œé™ç»´é˜¶æ®µã€‚

        å‚æ•°ï¼š
          - seq_len: â€¦
          - perplexity: â€¦
          - n_neighbors: â€¦
          - random_state: â€¦
          - pca_evp:    PCA ä¿ç•™æ–¹å·®æ¯”ä¾‹ï¼ˆ0< pca_evp <=1ï¼‰ï¼Œæˆ– None è·³è¿‡è¯¥æ–¹æ¡ˆï¼›
          - umap_components: UMAPâ†’t-SNE é¦–æ­¥è¾“å‡ºç»´åº¦ï¼ˆæ•´å‹ï¼‰ï¼Œæˆ– None è·³è¿‡è¯¥æ–¹æ¡ˆï¼›
        """
        # â€”â€”â€” 1. æå–å·²ç”Ÿæˆçš„ç‰¹å¾çŸ©é˜µ & æ—¶é—´ç´¢å¼• â€”â€”â€”
        df_feat = self.df_features.copy ()
        timestamps = df_feat['timestamp']
        X = df_feat.drop (columns=['timestamp']).values  # (T, d)
        T, d = X.shape

        # â€”â€”â€” 2. è®¡ç®—å‰å‘æ”¶ç›Š & å¯¹é½æ ‡ç­¾ â€”â€”â€”
        fp = self._eval_kwargs['forward_period']
        forward_ret = (
                self.df_global[self.target_col].shift (-fp)
                / self.df_global[self.target_col]
                - 1
        )
        aligned_forward = forward_ret.reindex (timestamps).reset_index (drop=True)

        # â€”â€”â€” 3. æ»‘çª—å±•å¼€ & æ ‡å‡†åŒ–ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰ â€”â€”â€”
        N = T - seq_len + 1
        V = np.empty ((N, seq_len * d), dtype=float)
        for i in tqdm (range (N), desc="ğŸ”„ çª—å£å±•å¹³"):
            V[i, :] = X[i:i + seq_len, :].flatten ()
        V = (V - V.mean (axis=0)) / (V.std (axis=0, ddof=0) + 1e-8)

        # â€”â€”â€” 4. ç”Ÿæˆæ¶¨è·Œæ ‡ç­¾ â€”â€”â€”
        labels = (aligned_forward.iloc[seq_len - 1: seq_len - 1 + N] >= 0)
        labels = labels.astype (int).to_numpy ()

        # â€”â€”â€” 5. å®šä¹‰é™ç»´ä»»åŠ¡åˆ—è¡¨ â€”â€”â€”
        reducers = [
            ('PCA-2', PCA (n_components=2, random_state=random_state)),
            ('LLE', LocallyLinearEmbedding (n_neighbors=n_neighbors, n_components=2, random_state=random_state)),
            ('t-SNE', TSNE (n_components=2, perplexity=perplexity, random_state=random_state)),
            (f'PCA({pca_evp * 100:.2f}%)â†’t-SNE', pca_evp),
            ('UMAP-2', umap.UMAP (n_neighbors=n_neighbors, n_components=2)),
            (f'UMAP({umap_components})â†’t-SNE', umap_components),
        ]
        results = {}

        # å¹¶è¡Œè®¡ç®—é™ç»´

        def _compute (item):
            name, algo = item
            if name == f'PCA({pca_evp * 100:.2f}%)â†’t-SNE':
                V_pca90 = PCA (n_components=algo, random_state=random_state).fit_transform (V)
                Z = TSNE (n_components=2, perplexity=perplexity, random_state=random_state).fit_transform (V_pca90)
            elif name == f'UMAP({umap_components})â†’t-SNE':
                V_umap_components = umap.UMAP (
                    n_neighbors=n_neighbors,
                    n_components=algo,
                ).fit_transform (V)
                Z = TSNE (n_components=2, perplexity=perplexity, random_state=random_state).fit_transform (
                    V_umap_components)
            else:
                Z = algo.fit_transform (V)
            return name, Z

        with ThreadPoolExecutor (max_workers=self.n_jobs) as executor:
            futures = {executor.submit (_compute, item): item for item in reducers}
            for fut in tqdm (as_completed (futures), total=len (futures), desc="ğŸ”„ å¹¶è¡Œé™ç»´"):
                name, Z = fut.result ()
                results[name] = Z

        # â€”â€”â€” 6. ç»˜å›¾ï¼ˆ2x3 ç½‘æ ¼ï¼Œä½¿ç”¨ constrained_layoutï¼‰ â€”â€”â€”
        fig, axes = plt.subplots (2, 3, figsize=(18, 10), constrained_layout=True)
        cm = plt.cm.coolwarm

        for ax, (title, Z) in zip (axes.flatten (), results.items ()):
            sc = ax.scatter (Z[:, 0], Z[:, 1], c=labels, cmap=cm, s=10, alpha=0.7)
            ax.set_title (title, fontsize=12)
            ax.set_xlabel ('Component 1')
            ax.set_ylabel ('Component 2')

        cbar = fig.colorbar (sc, ax=axes.ravel ().tolist (), ticks=[0, 1])
        cbar.ax.set_yticklabels (['Down', 'Up'])

        fig.suptitle (f'2D Structure Visualization (seq_len={seq_len})', fontsize=14)
        plt.show ()

    def evaluate_clusterings (
            self,
            algos: Optional[list] = None,
            param_grids: Optional[Dict[str, Dict[str, list]]] = None,
            metrics: Optional[list] = None,
            dim_reduction: str = 'none',
            reduction_params: Optional[Dict[str, Any]] = None,
            seq_len: int = 1,
    ) -> pd.DataFrame:
        """
        å¯¹å¤šç§èšç±»ç®—æ³•å’Œå‚æ•°ç»„åˆè¿›è¡Œè¯„ä¼°ï¼Œå¹¶å°†æœ€ä½³èšç±»æ ‡ç­¾å­˜å…¥ self.df_features['cluster']ã€‚

        å‚æ•°:
          - algos: Optional[Dict[str, Estimator]]ï¼Œç®—æ³•æ˜ å°„ï¼Œé”®ä¸ºç®—æ³•åç§°å­—ç¬¦ä¸²ï¼Œ
            å€¼ä¸ºèšç±»å™¨ç±»ï¼›æ”¯æŒçš„ç®—æ³•åç§°åŒ…æ‹¬ï¼š
            'KMeans', 'AgglomerativeClustering', 'SpectralClustering',
            'GaussianMixture', 'DBSCAN', 'MeanShift'ã€‚
            None è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰é»˜è®¤ç®—æ³•ã€‚

          - param_grids: Optional[Dict[str, Dict[str, list]]]ï¼Œè¶…å‚æ•°ç½‘æ ¼ï¼Œ
            é”®ä¸ algos ä¸­ç®—æ³•åç§°å¯¹åº”ï¼Œå€¼ä¸º {å‚æ•°å: å€™é€‰å€¼åˆ—è¡¨}ã€‚
            None è¡¨ç¤ºä½¿ç”¨é»˜è®¤ç½‘æ ¼ï¼š
              â€¢ KMeans: {'n_clusters': [2..10], 'random_state': [0]}
              â€¢ AgglomerativeClustering: {'n_clusters': [2..10], 'linkage': ['ward','average','complete']}
              â€¢ SpectralClustering: {'n_clusters': [2..10], 'affinity': ['rbf','nearest_neighbors'], 'random_state': [0]}
              â€¢ GaussianMixture: {'n_components': [2..10], 'covariance_type': ['full'], 'random_state': [0]}
              â€¢ DBSCAN: {'eps': [0.1,0.2,0.5], 'min_samples': [5,10,20]}
              â€¢ MeanShift: {'bandwidth': ['estimate']} (è‡ªåŠ¨ä¼°ç®—å¸¦å®½)

          - metrics: Optional[List[str]]ï¼Œè¯„ä¼°æŒ‡æ ‡åˆ—è¡¨ï¼Œå¯é€‰ï¼š
            'silhouette', 'calinski_harabasz', 'davies_bouldin'ã€‚
            None æ—¶ä½¿ç”¨ä¸‰è€…å…¨éƒ¨ã€‚

          - dim_reduction: strï¼Œé™ç»´æ–¹å¼ï¼Œ
            'none'ï¼ˆé»˜è®¤ï¼Œä¸é™ç»´ï¼‰ã€
            'pca'ï¼ˆPCA ä¿ç•™æ–¹å·®æ¯”ä¾‹ï¼‰ã€
            'umap'ï¼ˆUMAP é™ç»´ï¼‰ã€‚

          - reduction_params: Optional[Dict[str, Any]]ï¼Œé™ç»´å‚æ•°ï¼›
            å½“ dim_reduction='pca' æ—¶ï¼Œæ”¯æŒï¼š
              {'variance_ratio': float (0<var<=1)}ï¼›
            å½“ dim_reduction='umap' æ—¶ï¼Œæ”¯æŒï¼š
              {'n_components': int, 'n_neighbors': int, 'min_dist': float, ...}ã€‚
            None ä½¿ç”¨ç®—æ³•é»˜è®¤å‚æ•°ã€‚

          - seq_len: intï¼Œåºåˆ—çª—å£é•¿åº¦ï¼Œ
            å¤§äº 1 æ—¶å°†åŸå§‹ (T, d) æ•°æ®æŒ‰çª—å£å±•å¼€ä¸º
            (T-seq_len+1, seq_len*d) æ ·æœ¬ï¼Œæ ‡ç­¾å¯¹åº”çª—å£æœ«å°¾ timestampã€‚

        è¿”å›:
          - pd.DataFrameï¼šæ¯è¡Œå¯¹åº”ä¸€æ¬¡ç®—æ³•+å‚æ•°ç»„åˆçš„è¯„ä¼°ï¼Œ
            åˆ—åŒ…æ‹¬ 'algo', å„è¶…å‚æ•°, 'silhouette', 'calinski_harabasz', 'davies_bouldin'ã€‚
        """

        # é»˜è®¤ç®—æ³•ä¸å‚æ•°ç½‘æ ¼
        default_algos = {
            'KMeans': KMeans,
            'AgglomerativeClustering': AgglomerativeClustering,
            'SpectralClustering': SpectralClustering,
            'GaussianMixture': GaussianMixture,
            'DBSCAN': DBSCAN,
            'MeanShift': MeanShift,
        }
        default_param_grids = {
            'KMeans': {'n_clusters': list (range (2, 11)), 'random_state': [0]},
            'AgglomerativeClustering': {'n_clusters': list (range (2, 11)), 'linkage': ['ward', 'average', 'complete']},
            'SpectralClustering': {'n_clusters': list (range (2, 11)), 'affinity': ['rbf', 'nearest_neighbors'],
                                   'random_state': [0]},
            'GaussianMixture': {'n_components': list (range (2, 11)), 'covariance_type': ['full'], 'random_state': [0]},
            'DBSCAN': {'eps': [0.1, 0.2, 0.5], 'min_samples': [5, 10, 20]},
            'MeanShift': {'bandwidth': ['estimate']}
        }
        # è§£æ algos
        if algos is None:
            to_run = default_algos
        elif isinstance (algos, list):
            to_run = {name: default_algos[name] for name in algos if name in default_algos}
        else:
            raise ValueError ("`algos` must be None or list of algorithm names")
        # è§£æ param_grids
        if param_grids is None:
            grids = default_param_grids
        else:
            grids = {name: param_grids.get (name, default_param_grids[name]) for name in to_run}
        metrics = metrics or ['silhouette', 'calinski_harabasz', 'davies_bouldin']

        # æ„é€  X å’Œæ—¶é—´ç´¢å¼•
        df = self.df_features.drop (columns=['cluster'], errors='ignore').copy ()
        # ä»…ä¿ç•™å› å­ç‰¹å¾åˆ—ï¼Œæ’é™¤ timestamp å’Œ clusterï¼ˆé˜²æ­¢ NaN å¼•å…¥ï¼‰
        feature_cols = [c for c in df.columns if c not in ('timestamp', 'cluster')]
        X0 = df[feature_cols].values
        times = df['timestamp'].tolist ()
        if seq_len > 1:
            T, d = X0.shape
            N = T - seq_len + 1
            X = np.vstack ([X0[i:i + seq_len].flatten () for i in range (N)])
            time_index = [times[i + seq_len - 1] for i in range (N)]
        else:
            X = X0.copy ()
            time_index = times

        # æ ‡å‡†åŒ–
        scaler = (MinMaxScaler () if self._eval_kwargs['scaler'] == 'minmax'
                  else RobustScaler () if self._eval_kwargs['scaler'] == 'robust'
        else StandardScaler ())

        X = scaler.fit_transform (X)

        # é™ç»´
        reduction_params = reduction_params or {}
        if dim_reduction == 'pca':
            vr = reduction_params.get ('variance_ratio', 1.0)
            pca = PCA (n_components=vr, svd_solver='full', random_state=0)
            X = pca.fit_transform (X)
        elif dim_reduction == 'umap':
            n_c = reduction_params.get ('n_components', 2)
            umap_kwargs = {k: v for k, v in reduction_params.items () if k != 'n_components'}
            reducer = umap.UMAP (n_components=n_c, **umap_kwargs)
            X = reducer.fit_transform (X)

        # å¹¶è¡Œè¯„ä¼°
        tasks = [(name, Est, params, X, metrics)
                 for name, Est in to_run.items ()
                 for params in ParameterGrid (grids.get (name, {}))]
        records = []

        with ThreadPoolExecutor (max_workers=self.n_jobs) as executor:
            futures = {executor.submit (_evaluate_clustering_task, *task): task for task in tasks}
            iterator = as_completed (futures)
            iterator = tqdm (iterator, total=len (futures), desc='Clustering eval')
            for fut in iterator:
                rec = fut.result ()
                if rec:
                    records.append (rec)

        df_eval = pd.DataFrame.from_records (records)
        self.cluster_eval_ = df_eval.drop (columns=['_labels'], errors='ignore')

        # ç»¼åˆç»˜å›¾
        plt.figure ()
        best_sil = df_eval.groupby ('algo')['silhouette'].max ()
        best_sil.plot (kind='bar')
        plt.ylabel ('Max Silhouette')
        plt.title (f'Algorithm Comparison (dim_red={dim_reduction})')
        plt.show ()

        # silhouette plot
        best_idxs = df_eval.groupby ('algo')['silhouette'].idxmax ().items ()
        items = list (best_idxs)

        # æ¯ 3 ä¸ªç®—æ³•åšä¸€ç»„
        for i in range (0, len (items), 3):
            group = items[i:i + 3]
            n = len (group)
            # æ¯è¡Œ n åˆ—å­å›¾
            fig, axes = plt.subplots (1, n, figsize=(5 * n, 4), squeeze=False)
            for ax, (algo, idx) in zip (axes[0], group):
                labels = df_eval.loc[idx, '_labels']
                # å®‰å…¨è·å–ç°‡æ•°
                raw_nc = df_eval.loc[idx, 'n_clusters'] if 'n_clusters' in df_eval.columns else np.nan
                if pd.notna (raw_nc):
                    n_clusters = int (raw_nc)
                else:
                    unique_labels = np.unique (labels)
                    n_clusters = int ((unique_labels != -1).sum ())
                if n_clusters < 2:
                    ax.text (0.5, 0.5, f"{algo}\nåªæœ‰{n_clusters}ç°‡ï¼Œè·³è¿‡",
                             ha='center', va='center')
                    ax.axis ('off')
                    continue

                sil_vals = silhouette_samples (X, labels)
                y_lower = 10
                for j in range (n_clusters):
                    ith = np.sort (sil_vals[labels == j])
                    size_j = ith.shape[0]
                    y_upper = y_lower + size_j
                    ax.fill_betweenx (
                        np.arange (y_lower, y_upper),
                        0, ith, alpha=0.7
                    )
                    y_lower = y_upper + 10
                ax.set_title (f"{algo} (clusters={n_clusters})")
                ax.set_xlabel ("Silhouette")
                ax.set_ylabel ("Cluster label")
                ax.set_yticks ([])
                ax.axvline (df_eval.loc[idx, 'silhouette'], color='red', linestyle='--')

            plt.tight_layout ()
            plt.show ()

            self.cluster_report = df_eval

        return df_eval
